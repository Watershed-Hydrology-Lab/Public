{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0a034ac-18a4-4a46-92a8-b1d79aecc64a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Packages and Libraries Installation\n",
    "If this is your first time using Jupyter Notebook and you don't have the necessary packages installed, please install these packages and libraries. You only need to install these packages once. Uncomment the cell below and run it to install the packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb4b33a-6d7f-446e-bc5a-5cf9073f4e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --quiet pandas numpy matplotlib \n",
    "# !pip install --quiet pymannkendall\n",
    "# !pip install --quiet xclim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51a0fe0-9adb-4fe2-a7eb-f690506f4db4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Packages and Library importing\n",
    "`SOBRkendallTaubPtt` and `SOBRfig2025` are pyhton packages that are written to perform Mann-Kendall/Pettitt analysis and plot the necessary figures, respectively. These functions should be in your working directory to make it easy to work. Once you keep these in the working folder where your Jupyter notebooks are saved, you can import all these functions. You have to run below cell every time you run the code.\n",
    "The following are the import statements for these functions and other necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ff051f-8e10-4f2e-a7e1-1c4b63b292ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import SOBRkendallTaubPtt as mk\n",
    "import SOBRfig2025 as sb\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xclim\n",
    "import os\n",
    "import matplotlib.colors\n",
    "import scipy.stats as st\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import warnings; warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59f152e-bd38-446c-8027-915bd36e594d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Function Definitions for DataFrame Saving\n",
    "The following functions are defined to create a DataFrame to store the results obtained from the Pettitt test and Mann-Kendall test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2b8044",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def determine_trend_direction(Taub):\n",
    "    return \"Increasing\" if Taub > 0 else \"Decreasing\" if Taub < 0 else \"No trend\"\n",
    "\n",
    "def determine_trend_significance(p):\n",
    "    if p < 0.05:\n",
    "        return \"Highly Likely\"\n",
    "    elif 0.05 <= p < 0.10:\n",
    "        return \"Very Likely\"\n",
    "    elif 0.10 <= p < 0.34:\n",
    "        return \"Likely\"\n",
    "    else:\n",
    "        return \"Unlikely\"\n",
    "    \n",
    "def create_mk_result_entry(station_number, trend, slope, h, p, z, Taub, s, var_s, lb, ub,intercept):\n",
    "    trend_direction = determine_trend_direction(Taub)\n",
    "    trend_significance = determine_trend_significance(p)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'Station': [station_number],\n",
    "        'Trend': [trend],\n",
    "        'Trend Direction': [trend_direction],\n",
    "        'Trend Significance': [trend_significance],\n",
    "        'Hypothesis': ['trend.' if h else 'no trend.'],\n",
    "        'P-MK': [round(p, 7)],\n",
    "        'Z-score': [round(z, 7)],\n",
    "        'Taub': [round(Taub, 7)],\n",
    "        'S': [round(s, 7)],\n",
    "        'Variance of S': [round(var_s, 7)],\n",
    "        'Slope': [round(slope, 7)],\n",
    "        'Intercept': [round(intercept, 7)],\n",
    "        'Lower Bound': [lb],\n",
    "        'Upper Bound': [ub]\n",
    "    })\n",
    "\n",
    "def create_mk_result_entrytime(station_number, trend, slope, h, p, z, Taub, s, var_s, lb, ub,max_day_value,corresponding_day,intercept):\n",
    "    trend_direction = determine_trend_direction(slope)\n",
    "    trend_significance = determine_trend_significance(p)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'Station': [station_number],\n",
    "        'Trend': [trend],\n",
    "        'Trend Direction': [trend_direction],\n",
    "        'Trend Significance': [trend_significance],\n",
    "        'Hypothesis': ['trend.' if h else 'no trend.'],\n",
    "        'P-MK': [round(p, 7)],\n",
    "        'Z-score': [round(z, 7)],\n",
    "        'Taub': [round(Taub, 7)],\n",
    "        'S': [round(s, 7)],\n",
    "        'Variance of S': [round(var_s, 7)],\n",
    "        'Slope': [round(slope, 7)],\n",
    "        'Intercept': [round(intercept, 7)],\n",
    "        'Lower Bound': [lb],\n",
    "        'Upper Bound': [ub],\n",
    "        '3-day_max': [max_day_value],\n",
    "        'Day_of_year': [corresponding_day]\n",
    "    })\n",
    "\n",
    "def create_mk_result_entrytime7(station_number, trend, slope, h, p, z, Taub, s, var_s, lb, ub,min_day_value,corresponding_day,intercept):\n",
    "    trend_direction = determine_trend_direction(Taub)\n",
    "    trend_significance = determine_trend_significance(p)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'Station': [station_number],\n",
    "        'Trend': [trend],\n",
    "        'Trend Direction': [trend_direction],\n",
    "        'Trend Significance': [trend_significance],\n",
    "        'Hypothesis': ['trend.' if h else 'no trend.'],\n",
    "        'P-MK': [round(p, 7)],\n",
    "        'Z-score': [round(z, 7)],\n",
    "        'Taub': [round(Taub, 7)],\n",
    "        'S': [round(s, 7)],\n",
    "        'Variance of S': [round(var_s, 7)],\n",
    "        'Slope': [round(slope, 7)],\n",
    "        'Intercept': [round(intercept, 7)],\n",
    "        'Lower Bound': [lb],\n",
    "        'Upper Bound': [ub],\n",
    "        '7-day_min': [min_day_value],\n",
    "        'Day_of_year': [corresponding_day]\n",
    "    })\n",
    "\n",
    "def create_pettitt_result_entry(station, K, p_value, locP, q):\n",
    "    trend_significancePTT = determine_trend_significance(p_value)\n",
    "    return pd.DataFrame({\n",
    "        'Station': [station],\n",
    "        'Pettitt_K': [K],\n",
    "        'P-Ptt': [p_value],\n",
    "        'Trend SignificancePtt': [trend_significancePTT],\n",
    "        'Change value': [q.iloc[int(locP)]],\n",
    "        'Change Point': [q.index[int(locP)]]\n",
    "    })\n",
    "\n",
    "def create_pettitt_result_entrytime(station, K, p_value, locP, q):\n",
    "    trend_significancePTT = determine_trend_significance(p_value)\n",
    "    return pd.DataFrame({\n",
    "        'Station': [station],\n",
    "        'Pettitt_K': [K],\n",
    "        'P-Ptt': [p_value],\n",
    "        'Trend SignificancePtt': [trend_significancePTT],\n",
    "        'Change value': [q.iloc[int(locP)]],\n",
    "        'Change Point': [q.index[int(locP)]]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c33b47-9351-4479-8235-9a465b65d540",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ⚠️ Warning: File Path Slashes ⚠️\n",
    "\n",
    "When specifying file paths in your code, it's important to use the correct slash and be aware of how Python interprets them:\n",
    "\n",
    "- **Windows**: Although Windows traditionally uses backslashes (`\\\\`) in file paths, Python interprets a single backslash (`\\`) as an escape character. This can lead to errors. To avoid this, you can use either double backslashes (`\\\\`) or single forward slashes (`/`). For example:\n",
    "\n",
    "    ```python\n",
    "    # Using double backslashes\n",
    "    file_path = \"C:\\\\Users\\\\username\\\\Documents\\\\file.txt\"\n",
    "    \n",
    "    # Using single forward slashes\n",
    "    file_path = \"C:/Users/username/Documents/file.txt\"\n",
    "    ```\n",
    "\n",
    "    Alternatively, you can use raw strings, which treat the `\\` as a normal character rather than an escape character:\n",
    "\n",
    "    ```python\n",
    "    # Using raw strings\n",
    "    file_path = r\"C:\\Users\\username\\Documents\\file.txt\"\n",
    "    ```\n",
    "\n",
    "- **macOS and Linux**: These systems use forward slashes (`/`) for file paths. For example:\n",
    "\n",
    "    ```python\n",
    "    file_path = \"/Users/username/Documents/file.txt\"\n",
    "    ```\n",
    "\n",
    "If you encounter errors when running your code, check your file paths first. Make sure you're using the correct slash, and that the file path points to the correct location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e24ef97-20c9-4ca3-bc26-ef2af50d0b8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following files are needed to run the cells below and are provided with the code:\n",
    "\n",
    "1. **SOBR_on84.shp**: This is the shapefile of the Ontario province that will be used to plot the spatial figure to include stations.\n",
    "\n",
    "2. **Station_LatLongAr.csv**: This file contains the latitude, longitude, and area of all the HYDAT stations present in Ontario.\n",
    "\n",
    "Please ensure these files are in your working directory before proceeding.\n",
    "\n",
    "Change the path for directories where required. This is necessary when you're working with file inputs or outputs in your code. The exact path will depend on the file structure of your project and where your Jupyter notebook file is located.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68785b05-9248-4161-94a4-a7b5f09e79b7",
   "metadata": {},
   "source": [
    "In this code, `periods` is a list of tuples, where each tuple represents a period of interest. The `data` line is filtering the DataFrame to only include rows where the 'Date' is within the specified period. Remember to replace the dates with your actual period of interest.\n",
    "\n",
    "<!-- # Define the period of interest\n",
    "periods = [(1990, 2020)]\n",
    "# Filter data for the period\n",
    "data = data[(data['Date'] >= '1990-10-01') & (data['Date'] <= '2020-09-30')]   -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a56217b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Mean Annual Discharge\n",
    "1. It is determined by computing the arithmetic average of the daily streamflow values observed over the entire water year.\n",
    "2. Mann Kendall and Pettit test is applied for trend direction and change sigificance.\n",
    "3. Output will produce text files with results \n",
    "     - `meanQ_POR.csv`: This file contains the mean of each water year for the period of interest for each station.\n",
    "        The stations are arranged in columns and the rows are years in increasing order.\n",
    "    - `meanQ_station.csv`: This file contains the mean of each station over all the years.\n",
    "    - 'MKnP_MeanAnnualD.csv': This file contains the results of Man kendall and Pettit test for Mean annual discharge.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b75653a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a32c27c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db740a39",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "shapefile_path = r\"C:\\Users\\TIWARIDI\\OneDrive - Government of Ontario\\Documents\\provincial Boundary\\SOBR_on84.shp\"\n",
    "base_map = gpd.read_file(shapefile_path)\n",
    "data_folder = r\"C:\\Users\\TIWARIDI\\OneDrive - Government of Ontario\\Documents\\WSCdata\\SOBR_FinalStations\\SOBR_plot\\CNP1970_2020\"\n",
    "fig_folder1 = os.path.join(data_folder ,\"allSOBRplot\")\n",
    "\n",
    "\n",
    "# Create the \"figure\" folder if it doesn't exist\n",
    "os.makedirs(fig_folder1, exist_ok=True)\n",
    "\n",
    "\n",
    "mean_discharge_list = []\n",
    "expected_months = set(range(1, 13))\n",
    "# Directory containing the CSV files\n",
    "csv_directory = 'C:/Users/TIWARIDI/OneDrive - Government of Ontario/Documents/WSCdata/SOBR_FinalStations/SOBR_plot/CNP1970_2020'\n",
    "\n",
    "# Load the CSV files into dataframes\n",
    "coordinates_df = pd.read_csv(r'C:\\Users\\TIWARIDI\\OneDrive - Government of Ontario\\Documents\\WSCdata\\SOBR_FinalStations\\SOBR_plot\\Station_LatLongAr.csv', sep='\\t')\n",
    "\n",
    "\n",
    "# Create a new directory for results\n",
    "results_directory = 'C:/Users/TIWARIDI/OneDrive - Government of Ontario/Documents/WSCdata/SOBR_FinalStations/SOBR_plot/CNP1970_2020/Figurepettittann'\n",
    "if not os.path.exists(results_directory):\n",
    "    os.makedirs(results_directory)\n",
    "    \n",
    "fig_folderPTT = os.path.join(results_directory  ,\"PTT\")\n",
    "os.makedirs(fig_folderPTT, exist_ok=True)\n",
    "\n",
    "# Get a list of CSV files in the directory\n",
    "csv_files = [f for f in os.listdir(csv_directory) if f.endswith('.csv')]\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "results_df = pd.DataFrame(columns=['Station', 'Pettitt_K', 'P-Ptt','Trend SignificancePtt', 'Change value','Change Point'])\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "results_dfMAD = pd.DataFrame(columns=['Station', 'Trend','Trend Direction', 'Trend Significance','Hypothesis', 'P-MK', 'Z-score', 'Taub', 'S', 'Variance of S', 'Slope', 'Intercept','Lower Bound','Upper Bound'])\n",
    "\n",
    "# Define the period of interest\n",
    "periods = [(1970, 2020)]\n",
    "\n",
    "# Iterate over each CSV file\n",
    "for csv_file in csv_files:\n",
    "    # Read the CSV file\n",
    "    file_path = os.path.join(csv_directory, csv_file)\n",
    "    data = pd.read_csv(file_path)\n",
    "    data['Date'] = pd.to_datetime(data['Date'], format='%Y-%m-%d')\n",
    "    station_number = csv_file.split('.')[0].upper() \n",
    "\n",
    "    # Filter data for the period \n",
    "    data = data[(data['Date'] >= '1970-10-01') & (data['Date'] <= '2020-09-30')]\n",
    "\n",
    "    # Add a 'WaterYear' column to the data\n",
    "    data['WaterYear'] = data['Date'].dt.year + (data['Date'].dt.month >= 10)\n",
    "    \n",
    "    \n",
    "    for water_year, group in data.groupby('WaterYear'):\n",
    "        # Extract the months for this WaterYear\n",
    "        months_in_water_year = set(group['Date'].dt.month)\n",
    "\n",
    "        # Check if any month is missing\n",
    "        if months_in_water_year != expected_months:\n",
    "            # Replace only the 'value' column in this WaterYear group with NaN\n",
    "            data.loc[data['WaterYear'] == water_year, 'Value'] = np.nan\n",
    "\n",
    "\n",
    "    mean_discharge = data.groupby('WaterYear')['Value'].apply(lambda x: x.dropna().mean())\n",
    "\n",
    "    \n",
    "    \n",
    "    # Append the mean discharge for each water year to the list\n",
    "    mean_discharge_list.append(mean_discharge)\n",
    "    # Concatenate the list of mean discharge DataFrames into a single DataFrame\n",
    "    concatenated_results = pd.concat(mean_discharge_list, axis=1)\n",
    "\n",
    "   \n",
    "    \n",
    "    #perform men kendall test on mean annual discharge\n",
    "    trend, h, p, z, Tau, s, var_s, slope, intercept,lb,ub,Taub  = mk.yue_wang_modification_test(mean_discharge,lag=1)\n",
    "    new_result = create_mk_result_entry(csv_file.replace('.csv', ''), trend, slope, h, p, z, Taub, s, var_s, lb, ub,intercept)\n",
    "    results_dfMAD = pd.concat([results_dfMAD, new_result], ignore_index=True)\n",
    "\n",
    "    locP, K, p_value = mk.pettitt_dip(mean_discharge.values)\n",
    "    new_data = create_pettitt_result_entry(csv_file.replace('.csv', ''), K, p_value, locP, mean_discharge)\n",
    "    results_df = pd.concat([results_df, new_data], ignore_index=True)\n",
    "    \n",
    "    \n",
    "    ##################################################\n",
    "    # Calculate mean before and after the change point\n",
    "    change_point_year = mean_discharge.index[int(locP)]\n",
    "    mean_before_change = mean_discharge[mean_discharge.index < change_point_year].mean()\n",
    "    mean_after_change = mean_discharge[mean_discharge.index >= change_point_year].mean()\n",
    "    \n",
    " \n",
    "    plt.figure(figsize=(5, 4), facecolor='white')\n",
    "    plt.scatter(mean_discharge.index, mean_discharge, marker=\"o\", color=\"black\", s=8)\n",
    "    plt.plot(mean_discharge.index, slope * np.arange(len(mean_discharge)) + intercept, color='red',label='Trend Line')\n",
    "    plt.axvline(mean_discharge.index[int(locP)], color='b', linestyle='--', label='Change Point')\n",
    "    \n",
    "    # Reduce the size of tick marks on x and y axes\n",
    "    plt.xticks(fontsize=6)\n",
    "    plt.yticks(fontsize=6)\n",
    "  \n",
    "    # Plot horizontal line for the mean before the change point\n",
    "    plt.hlines(y=mean_before_change, xmin=mean_discharge.index.min(), xmax=change_point_year,color='green', linestyle='-', label=f'Mean Before ({mean_before_change:.3f})')\n",
    "\n",
    "    # Plot horizontal line for the mean after the change point\n",
    "    plt.hlines(y=mean_after_change, xmin=change_point_year, xmax=mean_discharge.index.max(),color='green', linestyle='-', label=f'Mean After ({mean_after_change:.3f})')\n",
    "\n",
    "\n",
    "    plt.xlabel(\"Year\",fontsize=6)\n",
    "    plt.ylabel('Discharge (m^3/sec)',fontsize=6)\n",
    "\n",
    "    plt.title(f\"Mean Annual Flow - Station {station_number}\",fontsize=6)             \n",
    "\n",
    "        \n",
    "    textstr = f\"p-MK: {p:.3f}\\nTau: {Tau:.3f}\\nSens-Slope: {slope:.3f}\"\n",
    "    props = dict(boxstyle='round', facecolor='white', alpha=0.5, edgecolor='none')\n",
    "    plt.text(1.01, 0.95, textstr, transform=plt.gca().transAxes, fontsize=6, verticalalignment='top', bbox=props)\n",
    "        \n",
    "    textstr = f\"p-Pettitt: {p_value:.3f}\\nChange point (CP): {mean_discharge.index[int(locP)]}\\nMean before CP:{mean_before_change:.2f}\\nMean after CP:{mean_after_change:.2f}\"\n",
    "    props = dict(boxstyle='round', facecolor='white', alpha=0.5, edgecolor='none')\n",
    "    plt.text(1.01, 0.80, textstr, transform=plt.gca().transAxes, fontsize=6, verticalalignment='top', bbox=props)\n",
    "        \n",
    "    plt.legend(['Discharge Data', 'Trend Line', 'Change Point'], bbox_to_anchor=(1.01, 0), loc='lower left',fontsize=6)\n",
    "\n",
    "    output_plot_filename = os.path.join(fig_folderPTT, f\"Station_{station_number}_plot_{periods[0][0]}-{periods[0][1]}png\")\n",
    "\n",
    "    plt.savefig(output_plot_filename, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    ##################################################\n",
    "         \n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "results_csv_file = os.path.join(results_directory, 'results.csv')\n",
    "results_df.to_csv(results_csv_file, index=False)\n",
    "\n",
    "# Merge the dataframes based on the station name\n",
    "merged_df1 = pd.merge( results_df, coordinates_df, left_on=\"Station\", right_on=\"Station\")\n",
    "# Define the path for the output file\n",
    "results_file_path1 = os.path.join(results_directory, 'results_latlon.csv')\n",
    "merged_df1 = merged_df1.drop_duplicates(subset='Station')\n",
    "\n",
    "merged_df1.to_csv(results_file_path1, index=False)\n",
    "\n",
    "\n",
    "# Merge the dataframes based on the station name\n",
    "merged_df = pd.merge( results_dfMAD, coordinates_df, left_on=\"Station\", right_on=\"Station\")\n",
    "# Define the path for the output file\n",
    "results_file_path = os.path.join(results_directory, 'resultsMAD_latlon.csv')\n",
    "merged_df = merged_df.drop_duplicates(subset='Station')\n",
    "merged_df.to_csv(results_file_path, index=False)\n",
    "\n",
    "column_means = concatenated_results.mean()\n",
    "\n",
    "mean_results = pd.DataFrame(column_means, columns=['Mean'])\n",
    "results_file_MEANQ = os.path.join(fig_folder1, 'meanQ_POR.csv')\n",
    "concatenated_results .to_csv(results_file_MEANQ, index=False)\n",
    "results_file_MEANstation = os.path.join(fig_folder1, 'meanQ_station.csv')\n",
    "mean_results .to_csv(results_file_MEANstation, index=False)\n",
    "\n",
    "#### adding %MAf to column\n",
    "mean_results_reset = mean_results.reset_index(drop=True)\n",
    "merged_df_reset = merged_df.reset_index(drop=True)\n",
    "\n",
    "# Concatenating DataFrames side by side\n",
    "merged_df11 = pd.concat([merged_df_reset, mean_results_reset], axis=1)\n",
    "# Creating a new column \"Slope as % of MAF\"\n",
    "merged_df11['Slope as % of MAF'] = abs((merged_df11['Slope'] / merged_df11['Mean']) * 100)\n",
    "\n",
    "\n",
    "# Assuming 'Station' is the common column in both dataframes\n",
    "combined_MKnP = results_df.merge(merged_df11, on='Station', how='outer')\n",
    "selected_columns = ['Station','Latitude', 'Longitude', 'Pettitt_K', 'P-Ptt','Trend SignificancePtt', 'Change value','Change Point','Trend Direction', 'Trend Significance', 'P-MK', 'Taub', 'Slope', 'Lower Bound','Upper Bound','Area','Mean','Slope as % of MAF']\n",
    "MKnP_MAD =combined_MKnP[selected_columns]\n",
    "MKnP_MAD.to_csv(os.path.join(fig_folder1, \"MKnP_MeanAnnualD.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45138f05-6c0e-44b3-8e65-4ebe8a8ba99f",
   "metadata": {},
   "source": [
    "The following lines of code are used to plot the figures for trend significance from the Mann-Kendall test and change significance from the Pettitt test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eae9fde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sb.plot_trend_mapPtt(merged_df1,shapefile_path,fig_folder1, 'Mean Annual Flow \\n (1971-2020) \\n Pettitt Significance', 'change_MAD.png')\n",
    "sb.plot_trend_mapMK(merged_df11,shapefile_path,fig_folder1, 'Mean Annual Flow \\n (1971-2020) \\n MK Significance', 'Trend_MAD.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3795025c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Annual 3 day Maximum Flow magnitude\n",
    "    - Calculated using a 3-day rolling window approach.\n",
    "    - MKnP_3daymaxflow.csv: This file contains the results of Man kendall and Pettit test for 3 day maximum flow magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d51ea6b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an empty DataFrame to store the results\n",
    "results_PT = pd.DataFrame(columns=['Station', 'Pettitt_K', 'P-Ptt','Trend SignificancePtt', 'Change value','Change Point'])\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "results_MK = pd.DataFrame(columns=['Station', 'Trend','Trend Direction', 'Trend Significance', 'Hypothesis', 'P-MK', 'Z-score', 'Taub', 'S', 'Variance of S', 'Slope', 'Intercept','Lower Bound','Upper Bound'])\n",
    "\n",
    "fig_folder = os.path.join(data_folder ,\"Figure3dayflow7cate\")\n",
    "\n",
    "# Create the \"figure\" folder if it doesn't exist\n",
    "os.makedirs(fig_folder, exist_ok=True)\n",
    "fig_folderPTT = os.path.join(fig_folder ,\"PTT\")\n",
    "os.makedirs(fig_folderPTT, exist_ok=True)\n",
    "\n",
    "# Get a list of all CSV files in the data folder\n",
    "csv_files = [file for file in os.listdir(data_folder) if file.endswith(\".csv\")]\n",
    "\n",
    "# Loop through each CSV file in the folder\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(data_folder, csv_file)\n",
    "    \n",
    "    # Load the data from the CSV file\n",
    "    data = pd.read_csv(file_path)\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    data.set_index('Date', inplace=True)\n",
    "    \n",
    "    for period in periods:\n",
    "        start_year, end_year = period\n",
    "\n",
    "    # Select data for the specified time period\n",
    "        station_data = data.loc[(data.index.year >= start_year) & (data.index.year <= end_year)]\n",
    "        \n",
    "        station_number = csv_file.split('.')[0].upper() \n",
    "        station_data =  station_data[ station_data['STATION_NUMBER'] == station_number].copy()\n",
    "        \n",
    " # Define the Water Year\n",
    "        station_data['Water_Year'] = station_data.index.where(station_data.index.month >= 10, station_data.index - pd.DateOffset(years=1)).year+1\n",
    "                 \n",
    "        station_data['3-day_rolling_max'] = station_data.groupby('Water_Year')['Value'].transform(lambda x: x.rolling(window=3, center=True,min_periods=1).mean())\n",
    "\n",
    "####################################\n",
    "        \n",
    "            # Group by WaterYear\n",
    "        for water_year, group in station_data.groupby('Water_Year'):\n",
    "            # Extract the months for this WaterYear\n",
    "#             months_in_water_year = set(group['Date'].dt.month)\n",
    "            months_in_water_year = set(group.index.month)\n",
    "#             print(months_in_water_year)\n",
    "\n",
    "            # Check if any month is missing\n",
    "            if months_in_water_year != expected_months:\n",
    "                # Replace only the 'value' column in this WaterYear group with NaN\n",
    "                station_data.loc[station_data['Water_Year'] == water_year, 'Value'] = np.nan\n",
    "\n",
    "        q_3daymaxn = station_data.groupby('Water_Year')['3-day_rolling_max'].apply(lambda x: x.dropna().max())\n",
    "        q_3daymax = q_3daymaxn.drop([q_3daymaxn.index[0], q_3daymaxn.index[-1]])\n",
    "     # Compute the Mann-Kendall trend\n",
    "\n",
    "    trend, h, p, z, Tau, s, var_s, slope, intercept,lb,ub,Taub  = mk.yue_wang_modification_test(q_3daymax,lag=1)\n",
    "\n",
    "    new_result = create_mk_result_entry(csv_file.replace('.csv', ''), trend, slope, h, p, z, Taub, s, var_s, lb, ub,intercept)\n",
    "    results_MK= pd.concat([results_MK, new_result], ignore_index=True)\n",
    "    \n",
    "    mask = ~np.isnan(q_3daymax.values)\n",
    "\n",
    "    locP, K, p_value = mk.pettitt_dip(q_3daymax.values[mask])\n",
    "    new_data = create_pettitt_result_entry(csv_file.replace('.csv', ''), K, p_value, locP, q_3daymax)\n",
    "    results_PT = pd.concat([results_PT, new_data], ignore_index=True)\n",
    "    \n",
    "    \n",
    "    ##################################################\n",
    "    # Calculate mean before and after the change point\n",
    "    change_point_year = q_3daymax.index[int(locP)]\n",
    "    mean_before_change = q_3daymax[q_3daymax.index < change_point_year].mean()\n",
    "    mean_after_change = q_3daymax[q_3daymax.index >= change_point_year].mean()\n",
    "\n",
    "    plt.figure(figsize=(5, 4), facecolor='white')\n",
    "    plt.scatter(q_3daymax.index, q_3daymax, marker=\"o\", color=\"black\", s=8)\n",
    "    plt.plot(q_3daymax.index, slope * np.arange(len(q_3daymax)) + intercept, color='red',label='Trend Line')\n",
    "    plt.axvline(q_3daymax.index[int(locP)], color='b', linestyle='--', label='Change Point')\n",
    "    \n",
    "    # Reduce the size of tick marks on x and y axes\n",
    "    plt.xticks(fontsize=6)\n",
    "    plt.yticks(fontsize=6)\n",
    "  \n",
    "    # Plot horizontal line for the mean before the change point\n",
    "    plt.hlines(y=mean_before_change, xmin=q_3daymax.index.min(), xmax=change_point_year,color='green', linestyle='-', label=f'Mean Before ({mean_before_change:.3f})')\n",
    "\n",
    "    # Plot horizontal line for the mean after the change point\n",
    "    plt.hlines(y=mean_after_change, xmin=change_point_year, xmax=q_3daymax.index.max(),color='green', linestyle='-', label=f'Mean After ({mean_after_change:.3f})')\n",
    "\n",
    "\n",
    "    plt.xlabel(\"Year\",fontsize=6)\n",
    "    plt.ylabel('Discharge (m^3/sec)',fontsize=6)\n",
    "    plt.title(f\"3 day max flow - Station {station_number}\",fontsize=6)\n",
    "        \n",
    "    textstr = f\"p-MK: {p:.3f}\\nTau: {Tau:.3f}\\nSens-Slope: {slope:.3f}\"\n",
    "    props = dict(boxstyle='round', facecolor='white', alpha=0.5, edgecolor='none')\n",
    "    plt.text(1.01, 0.95, textstr, transform=plt.gca().transAxes, fontsize=6, verticalalignment='top', bbox=props)\n",
    "        \n",
    "    textstr = f\"p-Pettitt: {p_value:.3f}\\nChange point (CP): {q_3daymax.index[int(locP)]}\\nMean before CP:{mean_before_change:.2f}\\nMean after CP:{mean_after_change:.2f}\"\n",
    "    props = dict(boxstyle='round', facecolor='white', alpha=0.5, edgecolor='none')\n",
    "    plt.text(1.01, 0.80, textstr, transform=plt.gca().transAxes, fontsize=6, verticalalignment='top', bbox=props)\n",
    "        \n",
    "    plt.legend(['Discharge Data', 'Trend Line', 'Change Point'], bbox_to_anchor=(1.01, 0), loc='lower left',fontsize=6)\n",
    "\n",
    "    output_plot_filename = os.path.join(fig_folderPTT, f\"Station_{station_number}_plot_{start_year}_{end_year}.png\")\n",
    "    plt.savefig(output_plot_filename, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    ##################################################\n",
    "         \n",
    "\n",
    "\n",
    "# Merge the dataframes based on the station name\n",
    "merged_df1 = pd.merge( results_PT, coordinates_df, left_on=\"Station\", right_on=\"Station\")\n",
    "# Define the path for the output file\n",
    "results_file_path1 = os.path.join(fig_folder, 'results_latlonPT.csv')\n",
    "merged_df1 = merged_df1.drop_duplicates(subset='Station')\n",
    "merged_df1.to_csv(results_file_path1, index=False)\n",
    "\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_file_path = os.path.join(fig_folder, 'results.csv')\n",
    "results_MK.to_csv(results_file_path, index=False)\n",
    "\n",
    "data_df =results_MK\n",
    "\n",
    "# Merge the dataframes based on the station name\n",
    "merged_df = pd.merge(data_df, coordinates_df, left_on=\"Station\", right_on=\"Station\")\n",
    "merged_df = merged_df.drop_duplicates(subset='Station')\n",
    "\n",
    "# Define the path for the output file\n",
    "results_file_path = os.path.join(fig_folder, 'results_latlonMK.csv')\n",
    "merged_df.to_csv(results_file_path, index=False)\n",
    "merged_df_reset = merged_df.reset_index(drop=True)\n",
    "mean_results_reset = mean_results.reset_index(drop=True)\n",
    "\n",
    "# Concatenating DataFrames side by side\n",
    "result_df_all = pd.concat([merged_df_reset, mean_results_reset], axis=1)\n",
    "# Creating a new column \"Slope as % of MAF\"\n",
    "result_df_all['Slope as % of MAF'] = abs((result_df_all['Slope'] / result_df_all['Mean']) * 100)\n",
    "\n",
    "\n",
    "# Define the path for the output file\n",
    "path = os.path.join(fig_folder, 'results_latlonMAF.csv')\n",
    "result_df_all.to_csv(path, index=False)\n",
    "\n",
    "merged_df11=result_df_all\n",
    "# Assuming 'Station' is the common column in both dataframes\n",
    "combined_MKnP = results_PT.merge(merged_df11, on='Station', how='outer')\n",
    "selected_columns = ['Station','Latitude', 'Longitude', 'Pettitt_K', 'P-Ptt','Trend SignificancePtt', 'Change value','Change Point','Trend Direction', 'Trend Significance', 'P-MK', 'Taub', 'Slope', 'Lower Bound','Upper Bound','Area','Mean','Slope as % of MAF']\n",
    "MKnP_MAD =combined_MKnP[selected_columns]\n",
    "MKnP_MAD.to_csv(os.path.join(fig_folder1, \"MKnP_3daymaxflow.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81091cbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sb.plot_trend_mapPtt(merged_df1, shapefile_path,fig_folder1,'3-day Maximum Flow \\n (1971-2020) \\n Pettitt Significance', 'change_3dayflow.png')\n",
    "sb.plot_trend_mapMK(merged_df11,shapefile_path,fig_folder1, '3-day Maximum Flow \\n (1971-2020) \\n MK Significance', 'trend_3dayflow.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a824244c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Annual 3 day maximum flow timing of Year\n",
    "    - Calculate 3-day rolling window approach and the corresponding timing is the 3-day maximum flow timing for the water year\n",
    "    - MKnP_3daymaxflowTime.csv: This file contains the results of Man kendall and Pettit test for 3 day maximum flow magnitude.\n",
    "    - result_df_all['slope over CNP'] = abs((result_df_all['Slope'] *30)) --------- In this line change the 30 to number of year to actual number of              years in the study, if is period of record then change to 50 , if climate normal period then keep 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9c1fa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an empty DataFrame to store the results\n",
    "results_PT = pd.DataFrame(columns=['Station', 'Pettitt_K', 'P-Ptt','Trend SignificancePtt', 'Change value','Change Point'])\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "results_MK = pd.DataFrame(columns=['Station', 'Trend','Trend Direction', 'Trend Significance', 'Hypothesis', 'P-MK', 'Z-score', 'Taub', 'S', 'Variance of S', 'Slope', 'Intercept','Lower Bound','Upper Bound','3-day_max','Day_of_year'])\n",
    "\n",
    "# Specify the path to the folder containing the discharge CSV files\n",
    "fig_folder = os.path.join(data_folder ,\"Figure3dayflowTime7cate\")\n",
    "# Create the \"figure\" folder if it doesn't exist\n",
    "os.makedirs(fig_folder, exist_ok=True)\n",
    "\n",
    "fig_folderPTT = os.path.join(fig_folder ,\"PTT\")\n",
    "os.makedirs(fig_folderPTT, exist_ok=True)\n",
    "\n",
    "# Get a list of all CSV files in the data folder\n",
    "csv_files = [file for file in os.listdir(data_folder) if file.endswith(\".csv\")]\n",
    "\n",
    "# Loop through each CSV file in the folder\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(data_folder, csv_file)\n",
    "    \n",
    "    # Load the data from the CSV file\n",
    "    data = pd.read_csv(file_path)\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    data.set_index('Date', inplace=True)\n",
    "    \n",
    "    for period in periods:\n",
    "        start_year, end_year = period\n",
    "\n",
    "    # Select data for the specified time period\n",
    "        station_data = data.loc[(data.index.year >= start_year) & (data.index.year <= end_year)]\n",
    "        #print(station_data)\n",
    "        \n",
    "        station_number = csv_file.split('.')[0].upper() \n",
    "       # print(station_number)\n",
    "        station_data =  station_data[ station_data['STATION_NUMBER'] == station_number].copy()\n",
    "\n",
    "        # Define the Water Year\n",
    "        station_data['Water_Year'] = station_data.index.where(station_data.index.month >= 10, station_data.index - pd.DateOffset(years=1)).year+1\n",
    "\n",
    "      \n",
    "\n",
    "        station_data['Water_Month'] = ((station_data.index.month + 2) % 12) + 1  \n",
    "        \n",
    "        \n",
    "        # Convert index to string for concatenation\n",
    "        index_str = station_data.index.strftime('%Y')\n",
    "\n",
    "        # Calculate water day\n",
    "        station_data['Water_Day'] = np.where(station_data.index.month >= 10,\n",
    "                                             (station_data.index - pd.to_datetime(index_str + \"-10-01\")).days + 1,\n",
    "                                             (station_data.index - pd.to_datetime((station_data.index.year - 1).astype(str) + \"-10-01\")).days + 1)\n",
    "\n",
    "        # Adjust for leap years\n",
    "        station_data['Water_Day'] = np.where((station_data.index.is_leap_year) & (station_data.index.month >= 10),\n",
    "                                             station_data['Water_Day'] + 1,\n",
    "                                             station_data['Water_Day'])\n",
    "\n",
    "\n",
    "#         print(station_data)\n",
    "        station_data['3-day_rolling_max'] = station_data.groupby('Water_Year')['Value'].transform(lambda x: x.rolling(window=3, center=True,min_periods=1).mean())\n",
    "#         print(station_data)\n",
    "\n",
    "      \n",
    "    ####################################\n",
    "        \n",
    "            # Group by WaterYear\n",
    "        for water_year, group in station_data.groupby('Water_Year'):\n",
    "\n",
    "            months_in_water_year = set(group.index.month)\n",
    "\n",
    "            # Check if any month is missing\n",
    "            if months_in_water_year != expected_months:\n",
    "                station_data.loc[station_data['Water_Year'] == water_year, 'Value'] = np.nan\n",
    "    \n",
    "    #####################################\n",
    "\n",
    "        max_flow_days_of_water_year = station_data.groupby('Water_Year')['3-day_rolling_max'].apply(lambda x: x.dropna().idxmax())\n",
    "\n",
    "\n",
    "        max_flow_days_of_water_year = max_flow_days_of_water_year.dropna()\n",
    "        max_flow_days_of_yearn = station_data.loc[max_flow_days_of_water_year]\n",
    "        max_flow_days_of_year = max_flow_days_of_yearn.drop([max_flow_days_of_yearn.index[0], max_flow_days_of_yearn.index[-1]])\n",
    "\n",
    "        max_flow_days_of_year['Day_of_year'] = max_flow_days_of_year.index.dayofyear\n",
    "#         print( max_flow_days_of_water_year)\n",
    "        max_flow_days_of_year['Month_of_year'] = max_flow_days_of_year.index.strftime('%m')\n",
    "        max_day_of_year = max_flow_days_of_year.loc[max_flow_days_of_year['3-day_rolling_max'].idxmax()]\n",
    "        max_day_value = max_day_of_year['3-day_rolling_max']\n",
    "        corresponding_month = max_day_of_year['Month_of_year'] \n",
    "        corresponding_day = max_day_of_year['Day_of_year'] \n",
    "\n",
    "    # Compute the Mann-Kendall trend\n",
    "    \n",
    "    trend, h, p, z, Tau, s, var_s, slope, intercept,lb,ub,Taub  = mk.yue_wang_modification_test(max_flow_days_of_year['Water_Day'],lag=1)\n",
    "\n",
    "    new_result = create_mk_result_entrytime(csv_file.replace('.csv', ''), trend, slope, h, p, z, Taub, s, var_s, lb, ub,max_day_value,corresponding_day,intercept)\n",
    "    results_MK= pd.concat([results_MK, new_result], ignore_index=True)\n",
    "    \n",
    "    mask = ~np.isnan(max_flow_days_of_year['Water_Day'].values)\n",
    "    x =max_flow_days_of_year['Water_Day'].values[mask]\n",
    "#     print(x)\n",
    "    xx=max_flow_days_of_year['Water_Day']\n",
    "    \n",
    "#     print(len(max_flow_days_of_year))\n",
    "\n",
    "    locP, K, p_value = mk.pettitt_dip(x)\n",
    "\n",
    "    new_data = create_pettitt_result_entrytime(csv_file.replace('.csv', ''), K, p_value, locP, xx)\n",
    "    results_PT = pd.concat([results_PT, new_data], ignore_index=True)\n",
    "    \n",
    "        ##################################################\n",
    "    # Calculate mean before and after the change point\n",
    "    change_point_year = max_flow_days_of_year['Water_Day'].index[int(locP)]\n",
    "    mean_before_change = max_flow_days_of_year['Water_Day'][max_flow_days_of_year['Water_Day'].index < change_point_year].mean()\n",
    "    mean_after_change = max_flow_days_of_year['Water_Day'][max_flow_days_of_year['Water_Day'].index >= change_point_year].mean()\n",
    "    \n",
    "#     print(mean_before_change, mean_after_change)\n",
    "  \n",
    "    \n",
    "    plt.figure(figsize=(5, 4), facecolor='white')\n",
    "    plt.scatter(max_flow_days_of_year['Water_Day'].index, max_flow_days_of_year['Water_Day'], marker=\"o\", color=\"black\", s=8)\n",
    "    plt.plot(max_flow_days_of_year['Water_Day'].index, slope * np.arange(len(max_flow_days_of_year['Water_Day'])) + intercept, color='red',label='Trend Line')\n",
    "    plt.axvline(max_flow_days_of_year['Water_Day'].index[int(locP)], color='b', linestyle='--', label='Change Point')\n",
    "    \n",
    "    # Reduce the size of tick marks on x and y axes\n",
    "    plt.xticks(fontsize=6)\n",
    "    plt.yticks(fontsize=6)\n",
    "  \n",
    "    # Plot horizontal line for the mean before the change point\n",
    "    plt.hlines(y=mean_before_change, xmin=max_flow_days_of_year['Water_Day'].index.min(), xmax=change_point_year,color='green', linestyle='-', label=f'Mean Before ({mean_before_change:.3f})')\n",
    "\n",
    "    # Plot horizontal line for the mean after the change point\n",
    "    plt.hlines(y=mean_after_change, xmin=change_point_year, xmax=max_flow_days_of_year['Water_Day'].index.max(),color='green', linestyle='-', label=f'Mean After ({mean_after_change:.3f})')\n",
    "\n",
    "\n",
    "    plt.xlabel(\"Water Year\",fontsize=6)\n",
    "    plt.ylabel('Water Days',fontsize=6)\n",
    "    plt.title(f\"3 day max flow Timing - Station {station_number}\",fontsize=6)\n",
    "    \n",
    "        \n",
    "    textstr = f\"p-MK: {p:.3f}\\nTau: {Tau:.3f}\\nSens-Slope: {slope:.3f}\"\n",
    "    props = dict(boxstyle='round', facecolor='white', alpha=0.5, edgecolor='none')\n",
    "    plt.text(1.01, 0.95, textstr, transform=plt.gca().transAxes, fontsize=6, verticalalignment='top', bbox=props)\n",
    "        \n",
    "    textstr = f\"p-Pettitt: {p_value:.3f}\\nChange point (CP): {max_flow_days_of_year['Water_Day'].index[int(locP)].year}\\nMean before CP:{mean_before_change:.2f}\\nMean after CP:{mean_after_change:.2f}\"\n",
    "    props = dict(boxstyle='round', facecolor='white', alpha=0.5, edgecolor='none')\n",
    "    plt.text(1.01, 0.80, textstr, transform=plt.gca().transAxes, fontsize=6, verticalalignment='top', bbox=props)\n",
    "        \n",
    "    plt.legend(['Discharge Data', 'Trend Line', 'Change Point'], bbox_to_anchor=(1.01, 0), loc='lower left',fontsize=6)\n",
    "\n",
    "    output_plot_filename = os.path.join(fig_folderPTT, f\"Station_{station_number}_plot_{start_year+1}_{end_year}.png\")\n",
    "    plt.savefig(output_plot_filename, bbox_inches='tight', dpi=300)\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "    ##################################################\n",
    "\n",
    "    \n",
    "# Merge the dataframes based on the station name\n",
    "merged_df1 = pd.merge( results_PT, coordinates_df, left_on=\"Station\", right_on=\"Station\")\n",
    "# Define the path for the output file\n",
    "results_file_path1 = os.path.join(fig_folder, 'results_latlonPT.csv')\n",
    "merged_df1 = merged_df1.drop_duplicates(subset='Station')\n",
    "merged_df1.to_csv(results_file_path1, index=False)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_file_path = os.path.join(fig_folder, 'results.csv')\n",
    "results_MK.to_csv(results_file_path, index=False)\n",
    "\n",
    "data_df =results_MK\n",
    "\n",
    "# Merge the dataframes based on the station name\n",
    "merged_df = pd.merge(data_df, coordinates_df, left_on=\"Station\", right_on=\"Station\")\n",
    "merged_df = merged_df.drop_duplicates(subset='Station')\n",
    "\n",
    "# Define the path for the output file\n",
    "results_file_path = os.path.join(fig_folder, 'results_latlonMK.csv')\n",
    "merged_df.to_csv(results_file_path, index=False)\n",
    "merged_df_reset = merged_df.reset_index(drop=True)\n",
    "mean_results_reset = mean_results.reset_index(drop=True)\n",
    "\n",
    "# Concatenating DataFrames side by side\n",
    "result_df_all = pd.concat([merged_df_reset, mean_results_reset], axis=1)\n",
    "# Creating a new column \"slope over CNP\"\n",
    "result_df_all['slope over CNP'] = abs((result_df_all['Slope'] *50))\n",
    "\n",
    "path = os.path.join(fig_folder, 'results_latlonMAF.csv')\n",
    "result_df_all.to_csv(path, index=False)\n",
    "\n",
    "merged_df11=result_df_all\n",
    "# Assuming 'Station' is the common column in both dataframes\n",
    "combined_MKnP = results_PT.merge(merged_df11, on='Station', how='outer')\n",
    "selected_columns = ['Station','Latitude', 'Longitude', 'Pettitt_K', 'P-Ptt','Trend SignificancePtt', 'Change value','Change Point','Trend Direction', 'Trend Significance', 'P-MK', 'Taub', 'Slope', 'Lower Bound','Upper Bound','Area','Mean','slope over CNP']\n",
    "MKnP_MAD =combined_MKnP[selected_columns]\n",
    "MKnP_MAD.to_csv(os.path.join(fig_folder1, \"MKnP_3daymaxflowTime.csv\"), index=False)\n",
    "## plot\n",
    "\n",
    "sb.plot_trend_mapPtt(merged_df1, shapefile_path,fig_folder1,'3-day Maximum Flow Timing\\n (1971-2020) \\n Pettitt Significance', 'change_3dayflowTime.png')\n",
    "sb.plot_trend_mapMKT(merged_df11,shapefile_path,fig_folder1, '3-day Maximum Flow Timing\\n (1971-2020) \\n MK Significance', 'trend_3dayflowTime.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06971b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baa0d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25919489",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Annual 7 day Minimum Flow magnitude\n",
    "    - Calculated using a 7-day rolling window approach.\n",
    "    - MKnP_7dayminflow.csv: This file contains the results of Man kendall and Pettit test for 7 day minimum flow magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a459020",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an empty DataFrame to store the results\n",
    "results_PT = pd.DataFrame(columns=['Station', 'Pettitt_K', 'P-Ptt','Trend SignificancePtt', 'Change value','Change Point'])\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "results_MK = pd.DataFrame(columns=['Station', 'Trend','Trend Direction', 'Trend Significance', 'Hypothesis', 'P-MK', 'Z-score', 'Taub', 'S', 'Variance of S', 'Slope', 'Intercept','Lower Bound','Upper Bound'])\n",
    "\n",
    "# Specify the path to the folder containing the discharge CSV files\n",
    "fig_folder = os.path.join(data_folder ,\"Figure7dayflow7cate\")\n",
    "\n",
    "# Create the \"figure\" folder if it doesn't exist\n",
    "os.makedirs(fig_folder, exist_ok=True)\n",
    "\n",
    "fig_folderPTT = os.path.join(fig_folder ,\"PTT\")\n",
    "os.makedirs(fig_folderPTT, exist_ok=True)\n",
    "\n",
    "\n",
    "# Get a list of all CSV files in the data folder\n",
    "csv_files = [file for file in os.listdir(data_folder) if file.endswith(\".csv\")]\n",
    "\n",
    "# Loop through each CSV file in the folder\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(data_folder, csv_file)\n",
    "    \n",
    "    # Load the data from the CSV file\n",
    "    data = pd.read_csv(file_path)\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    data.set_index('Date', inplace=True)\n",
    "    \n",
    "    for period in periods:\n",
    "        start_year, end_year = period\n",
    "\n",
    "    # Select data for the specified time period\n",
    "        station_data = data.loc[(data.index.year >= start_year) & (data.index.year <= end_year)]\n",
    "        #print(station_data)\n",
    "        \n",
    "        station_number = csv_file.split('.')[0].upper() \n",
    "       # print(station_number)\n",
    "        station_data =  station_data[ station_data['STATION_NUMBER'] == station_number].copy()\n",
    "        \n",
    "         # Define the Water Year\n",
    "        station_data['Water_Year'] = station_data.index.where(station_data.index.month >= 10, station_data.index - pd.DateOffset(years=1)).year+1\n",
    "        \n",
    "                 \n",
    "        station_data['7-day_rolling_min'] = station_data.groupby('Water_Year')['Value'].transform(lambda x: x.rolling(window=7, center=True,min_periods=1).mean())\n",
    "\n",
    "            # Group by WaterYear\n",
    "        for water_year, group in station_data.groupby('Water_Year'):\n",
    "\n",
    "            months_in_water_year = set(group.index.month)\n",
    "\n",
    "            # Check if any month is missing\n",
    "            if months_in_water_year != expected_months:\n",
    "                # Replace only the 'value' column in this WaterYear group with NaN\n",
    "                station_data.loc[station_data['Water_Year'] == water_year, 'Value'] = np.nan\n",
    "\n",
    "    \n",
    "    #####################################\n",
    "\n",
    "        q_7dayminn = station_data[station_data['7-day_rolling_min'] > 0].groupby('Water_Year')['7-day_rolling_min'].apply(lambda x: x.dropna().min())\n",
    "\n",
    "    \n",
    "        q_7daymin = q_7dayminn.drop([q_7dayminn.index[0], q_7dayminn.index[-1]])\n",
    "\n",
    "    # Perform the Mann-Kendall test\n",
    "   \n",
    "    trend, h, p, z, Tau, s, var_s, slope, intercept,lb,ub,Taub  = mk.yue_wang_modification_test(q_7daymin,lag=1)\n",
    "\n",
    "    new_result = create_mk_result_entry(csv_file.replace('.csv', ''), trend, slope, h, p, z, Taub, s, var_s, lb, ub,intercept)\n",
    "    results_MK= pd.concat([results_MK, new_result], ignore_index=True)\n",
    "    \n",
    "    mask = ~np.isnan(q_7daymin.values)\n",
    "\n",
    "    locP, K, p_value = mk.pettitt_dip(q_7daymin.values[mask])\n",
    "    new_data = create_pettitt_result_entry(csv_file.replace('.csv', ''), K, p_value, locP, q_7daymin)\n",
    "    results_PT = pd.concat([results_PT, new_data], ignore_index=True)\n",
    "    \n",
    "        ##################################################\n",
    "    # Calculate mean before and after the change point\n",
    "    change_point_year = q_7daymin.index[int(locP)]\n",
    "    mean_before_change = q_7daymin[q_7daymin.index < change_point_year].mean()\n",
    "    mean_after_change = q_7daymin[q_7daymin.index >= change_point_year].mean()\n",
    "    \n",
    "#     print(mean_before_change, mean_after_change)\n",
    "  \n",
    "    \n",
    "    plt.figure(figsize=(5, 4), facecolor='white')\n",
    "    plt.scatter(q_7daymin.index, q_7daymin, marker=\"o\", color=\"black\", s=8)\n",
    "    plt.plot(q_7daymin.index, slope * np.arange(len(q_7daymin)) + intercept, color='red',label='Trend Line')\n",
    "    plt.axvline(q_7daymin.index[int(locP)], color='b', linestyle='--', label='Change Point')\n",
    "    \n",
    "    # Reduce the size of tick marks on x and y axes\n",
    "    plt.xticks(fontsize=6)\n",
    "    plt.yticks(fontsize=6)\n",
    "  \n",
    "    # Plot horizontal line for the mean before the change point\n",
    "    plt.hlines(y=mean_before_change, xmin=q_7daymin.index.min(), xmax=change_point_year,color='green', linestyle='-', label=f'Mean Before ({mean_before_change:.3f})')\n",
    "\n",
    "    # Plot horizontal line for the mean after the change point\n",
    "    plt.hlines(y=mean_after_change, xmin=change_point_year, xmax=q_7daymin.index.max(),color='green', linestyle='-', label=f'Mean After ({mean_after_change:.3f})')\n",
    "\n",
    "\n",
    "    plt.xlabel(\"Water Year\",fontsize=6)\n",
    "    plt.ylabel('Discharge (m^3/sec)',fontsize=6)\n",
    "    plt.title(f\"7 day min flow - Station {station_number}\",fontsize=6)\n",
    "        \n",
    "    textstr = f\"p-MK: {p:.3f}\\nTau: {Tau:.3f}\\nSens-Slope: {slope:.3f}\"\n",
    "    props = dict(boxstyle='round', facecolor='white', alpha=0.5, edgecolor='none')\n",
    "    plt.text(1.01, 0.95, textstr, transform=plt.gca().transAxes, fontsize=6, verticalalignment='top', bbox=props)\n",
    "        \n",
    "    textstr = f\"p-Pettitt: {p_value:.3f}\\nChange point (CP): {q_7daymin.index[int(locP)]}\\nMean before CP:{mean_before_change:.2f}\\nMean after CP:{mean_after_change:.2f}\"\n",
    "    props = dict(boxstyle='round', facecolor='white', alpha=0.5, edgecolor='none')\n",
    "    plt.text(1.01, 0.80, textstr, transform=plt.gca().transAxes, fontsize=6, verticalalignment='top', bbox=props)\n",
    "        \n",
    "    plt.legend(['Discharge Data', 'Trend Line', 'Change Point'], bbox_to_anchor=(1.01, 0), loc='lower left',fontsize=6)\n",
    "\n",
    "    output_plot_filename = os.path.join(fig_folderPTT, f\"Station_{station_number}_plot_{start_year+1}_{end_year}.png\")\n",
    "    plt.savefig(output_plot_filename, bbox_inches='tight', dpi=300)\n",
    "\n",
    "\n",
    "    ##################################################\n",
    "         \n",
    "\n",
    "     \n",
    " # Merge the dataframes based on the station name\n",
    "merged_df1 = pd.merge( results_PT, coordinates_df, left_on=\"Station\", right_on=\"Station\")\n",
    "# Define the path for the output file\n",
    "results_file_path1 = os.path.join(fig_folder, 'results_latlonPT.csv')\n",
    "merged_df1 = merged_df1.drop_duplicates(subset='Station')\n",
    "merged_df1.to_csv(results_file_path1, index=False)\n",
    "\n",
    "\n",
    "data_df =results_MK\n",
    "\n",
    "# Merge the dataframes based on the station name\n",
    "merged_df = pd.merge(data_df, coordinates_df, left_on=\"Station\", right_on=\"Station\")\n",
    "merged_df = merged_df.drop_duplicates(subset='Station')\n",
    "\n",
    "# Define the path for the output file\n",
    "results_file_path = os.path.join(fig_folder, 'results_latlonMK.csv')\n",
    "merged_df.to_csv(results_file_path, index=False)\n",
    "merged_df_reset = merged_df.reset_index(drop=True)\n",
    "mean_results_reset = mean_results.reset_index(drop=True)\n",
    "\n",
    "# Concatenating DataFrames side by side\n",
    "result_df_all = pd.concat([merged_df_reset, mean_results_reset], axis=1)\n",
    "# Creating a new column \"Slope as % of MAF\"\n",
    "result_df_all['Slope as % of MAF'] = abs((result_df_all['Slope'] / result_df_all['Mean']) * 100)\n",
    "\n",
    "\n",
    "# Define the path for the output file\n",
    "path = os.path.join(fig_folder, 'results_latlonMAF.csv')\n",
    "result_df_all.to_csv(path, index=False)\n",
    "\n",
    "merged_df11=result_df_all\n",
    "# Assuming 'Station' is the common column in both dataframes\n",
    "combined_MKnP = results_PT.merge(merged_df11, on='Station', how='outer')\n",
    "selected_columns = ['Station','Latitude', 'Longitude', 'Pettitt_K', 'P-Ptt','Trend SignificancePtt', 'Change value','Change Point','Trend Direction', 'Trend Significance', 'P-MK', 'Taub', 'Slope', 'Lower Bound','Upper Bound','Area','Mean','Slope as % of MAF']\n",
    "MKnP_MAD =combined_MKnP[selected_columns]\n",
    "MKnP_MAD.to_csv(os.path.join(fig_folder1, \"MKnP_7dayminflow.csv\"), index=False)\n",
    "   \n",
    "## plot\n",
    "sb.plot_trend_mapPtt(merged_df1, shapefile_path,fig_folder1,'7-day Minimum Flow \\n (1971-2020) \\n Pettitt Significance', 'change_7dayflow.png')\n",
    "sb.plot_trend_mapMK(merged_df11,shapefile_path,fig_folder1, '7-day Minimum Flow \\n (1971-2020) \\n MK Significance', 'trend_7dayflow.png')   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e035a822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5926c315",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Annual 7 day Minimum Flow timing\n",
    "    - Calculate 7-day rolling window approach and the corresponding timing is the 7-day minimum flow timing for the water year\n",
    "    - MKnP_7dayminflowTime.csv: This file contains the results of Man kendall and Pettit test for 7 day minimum flow magnitude.\n",
    "      - result_df_all['slope over CNP'] = abs((result_df_all['Slope'] *30)) --------- In this line change the 30 to number of year to actual number of              years in the study, if is period of record then change to 50 , if climate normal period then keep 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43cfddf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an empty DataFrame to store the results\n",
    "results_PT = pd.DataFrame(columns=['Station', 'Pettitt_K', 'P-Ptt','Trend SignificancePtt', 'Change value','Change Point'])\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "results_MK = pd.DataFrame(columns=['Station', 'Trend','Trend Direction', 'Trend Significance', 'Hypothesis', 'P-MK', 'Z-score', 'Taub', 'S', 'Variance of S', 'Slope', 'Intercept','Lower Bound','Upper Bound','7-day_min','Day_of_year'])\n",
    "\n",
    "# Specify the path to the folder containing the discharge CSV files\n",
    "fig_folder = os.path.join(data_folder ,\"Figure7dayflowTime7cate\")\n",
    "\n",
    "# Create the \"figure\" folder if it doesn't exist\n",
    "os.makedirs(fig_folder, exist_ok=True)\n",
    "\n",
    "fig_folderPTT = os.path.join(fig_folder ,\"PTT\")\n",
    "os.makedirs(fig_folderPTT, exist_ok=True)\n",
    "\n",
    "\n",
    "# Get a list of all CSV files in the data folder\n",
    "csv_files = [file for file in os.listdir(data_folder) if file.endswith(\".csv\")]\n",
    "\n",
    "# Loop through each CSV file in the folder\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(data_folder, csv_file)\n",
    "    \n",
    "    # Load the data from the CSV file\n",
    "    data = pd.read_csv(file_path)\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    data.set_index('Date', inplace=True)\n",
    "    \n",
    "    for period in periods:\n",
    "        start_year, end_year = period\n",
    "\n",
    "    # Select data for the specified time period\n",
    "        station_data = data.loc[(data.index.year >= start_year) & (data.index.year <= end_year)]\n",
    "        #print(station_data)\n",
    "        \n",
    "        station_number = csv_file.split('.')[0].upper() \n",
    "#         print(station_number)\n",
    "        station_data =  station_data[ station_data['STATION_NUMBER'] == station_number].copy()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Define the Water Year\n",
    "        station_data['Water_Year'] = station_data.index.where(station_data.index.month >= 10, station_data.index - pd.DateOffset(years=1)).year+1\n",
    "        \n",
    "        \n",
    "        station_data['Water_Month'] = ((station_data.index.month + 2) % 12) + 1  \n",
    "        \n",
    "        \n",
    "        # Convert index to string for concatenation\n",
    "        index_str = station_data.index.strftime('%Y')\n",
    "\n",
    "        # Calculate water day\n",
    "        station_data['Water_Day'] = np.where(station_data.index.month >= 10,\n",
    "                                             (station_data.index - pd.to_datetime(index_str + \"-10-01\")).days + 1,\n",
    "                                             (station_data.index - pd.to_datetime((station_data.index.year - 1).astype(str) + \"-10-01\")).days + 1)\n",
    "\n",
    "        # Adjust for leap years\n",
    "        station_data['Water_Day'] = np.where((station_data.index.is_leap_year) & (station_data.index.month >= 10),\n",
    "                                             station_data['Water_Day'] + 1,\n",
    "                                             station_data['Water_Day'])\n",
    "\n",
    "\n",
    "            \n",
    "        station_data['7-day_rolling_min'] = station_data.groupby('Water_Year')['Value'].transform(lambda x: x.rolling(window=7, center=True,min_periods=1).mean())\n",
    "#         print(station_data)\n",
    "\n",
    "        ####################################\n",
    "        \n",
    "            # Group by WaterYear\n",
    "        for water_year, group in station_data.groupby('Water_Year'):\n",
    "            # Extract the months for this WaterYear\n",
    "#             months_in_water_year = set(group['Date'].dt.month)\n",
    "            months_in_water_year = set(group.index.month)\n",
    "#             print(months_in_water_year)\n",
    "\n",
    "            # Check if any month is missing\n",
    "            if months_in_water_year != expected_months:\n",
    "                # Replace only the 'value' column in this WaterYear group with NaN\n",
    "                station_data.loc[station_data['Water_Year'] == water_year, 'Value'] = np.nan\n",
    "#                 print(station_data['Value'])\n",
    "    \n",
    "    #####################################\n",
    "      \n",
    "\n",
    "\n",
    "        filtered_data = station_data[station_data['7-day_rolling_min'] > 0]\n",
    "        min_flow_days_of_water_year = filtered_data.groupby('Water_Year')['7-day_rolling_min'].apply(lambda x: x.dropna().idxmin())\n",
    "\n",
    "         \n",
    "\n",
    "        # Filter out missing values in the resulting index\n",
    "        min_flow_days_of_water_year = min_flow_days_of_water_year.dropna()\n",
    "        min_flow_days_of_yearn = station_data.loc[min_flow_days_of_water_year]\n",
    "        min_flow_days_of_year = min_flow_days_of_yearn.drop([min_flow_days_of_yearn.index[0], min_flow_days_of_yearn.index[-1]])\n",
    "        min_flow_days_of_year['Day_of_year'] = min_flow_days_of_year.index.dayofyear\n",
    "    \n",
    "        min_flow_days_of_year['Month_of_year'] = min_flow_days_of_year.index.strftime('%m')\n",
    "        min_day_of_year = min_flow_days_of_year.loc[min_flow_days_of_year['7-day_rolling_min'].idxmin()]\n",
    "        min_day_value = min_day_of_year['7-day_rolling_min']\n",
    "        corresponding_month = min_day_of_year['Month_of_year'] \n",
    "        corresponding_day = min_day_of_year['Day_of_year'] \n",
    "\n",
    "\n",
    "    # Perform the Mann-Kendall test\n",
    "   \n",
    "\n",
    "    trend, h, p, z, Tau, s, var_s, slope, intercept,lb,ub,Taub  = mk.yue_wang_modification_test(min_flow_days_of_year['Water_Day'],lag=1)\n",
    "\n",
    "    new_result = create_mk_result_entrytime7(csv_file.replace('.csv', ''), trend, slope, h, p, z, Taub, s, var_s, lb, ub,min_day_value,corresponding_day,intercept)\n",
    "    results_MK= pd.concat([results_MK, new_result], ignore_index=True)\n",
    "    \n",
    "    mask = ~np.isnan(min_flow_days_of_year['Water_Day'].values)\n",
    "    x =min_flow_days_of_year['Water_Day'].values[mask]\n",
    "    xx=min_flow_days_of_year['Water_Day']\n",
    "#     print((x))\n",
    " \n",
    "\n",
    "    locP, K, p_value = mk.pettitt_dip(min_flow_days_of_year['Water_Day'].values[mask])\n",
    "    new_data = create_pettitt_result_entrytime(csv_file.replace('.csv', ''), K, p_value, locP, xx)\n",
    "    results_PT = pd.concat([results_PT, new_data], ignore_index=True)\n",
    "    \n",
    "        ##################################################\n",
    "    # Calculate mean before and after the change point\n",
    "    change_point_year = min_flow_days_of_year['Water_Day'].index[int(locP)]\n",
    "    mean_before_change = min_flow_days_of_year['Water_Day'][min_flow_days_of_year['Water_Day'].index < change_point_year].mean()\n",
    "    mean_after_change = min_flow_days_of_year['Water_Day'][min_flow_days_of_year['Water_Day'].index >= change_point_year].mean()\n",
    "    \n",
    " \n",
    "    \n",
    "    plt.figure(figsize=(5, 4), facecolor='white')\n",
    "    plt.scatter(min_flow_days_of_year['Water_Day'].index, min_flow_days_of_year['Water_Day'], marker=\"o\", color=\"black\", s=8)\n",
    "    plt.plot(min_flow_days_of_year['Water_Day'].index, slope * np.arange(len(min_flow_days_of_year['Water_Day'])) + intercept, color='red',label='Trend Line')\n",
    "    plt.axvline(min_flow_days_of_year['Water_Day'].index[int(locP)], color='b', linestyle='--', label='Change Point')\n",
    "    \n",
    "    # Reduce the size of tick marks on x and y axes\n",
    "    plt.xticks(fontsize=6)\n",
    "    plt.yticks(fontsize=6)\n",
    "  \n",
    "    # Plot horizontal line for the mean before the change point\n",
    "    plt.hlines(y=mean_before_change, xmin=min_flow_days_of_year['Water_Day'].index.min(), xmax=change_point_year,color='green', linestyle='-', label=f'Mean Before ({mean_before_change:.3f})')\n",
    "\n",
    "    # Plot horizontal line for the mean after the change point\n",
    "    plt.hlines(y=mean_after_change, xmin=change_point_year, xmax=min_flow_days_of_year['Water_Day'].index.max(),color='green', linestyle='-', label=f'Mean After ({mean_after_change:.3f})')\n",
    "\n",
    "\n",
    "    plt.xlabel(\"Water Year\",fontsize=6)\n",
    "    plt.ylabel('Water Day)',fontsize=6)\n",
    "    plt.title(f\"7 day min flow Timing - Station {station_number} \",fontsize=6)\n",
    "       \n",
    "    textstr = f\"p-MK: {p:.3f}\\nTau: {Tau:.3f}\\nSens-Slope: {slope:.3f}\"\n",
    "    props = dict(boxstyle='round', facecolor='white', alpha=0.5, edgecolor='none')\n",
    "    plt.text(1.01, 0.95, textstr, transform=plt.gca().transAxes, fontsize=6, verticalalignment='top', bbox=props)\n",
    "        \n",
    "    textstr = f\"p-Pettitt: {p_value:.3f}\\nChange point (CP): {min_flow_days_of_year['Water_Day'].index[int(locP)].year}\\nMean before CP:{mean_before_change:.2f}\\nMean after CP:{mean_after_change:.2f}\"\n",
    "    props = dict(boxstyle='round', facecolor='white', alpha=0.5, edgecolor='none')\n",
    "    plt.text(1.01, 0.80, textstr, transform=plt.gca().transAxes, fontsize=6, verticalalignment='top', bbox=props)\n",
    "        \n",
    "    plt.legend(['Discharge Data', 'Trend Line', 'Change Point'], bbox_to_anchor=(1.01, 0), loc='lower left',fontsize=6)\n",
    "\n",
    "    output_plot_filename = os.path.join(fig_folderPTT, f\"Station_{station_number}_plot_{start_year+1}_{end_year}.png\")\n",
    "    plt.savefig(output_plot_filename, bbox_inches='tight', dpi=300)\n",
    "\n",
    "\n",
    "    ##################################################\n",
    "         \n",
    "\n",
    "\n",
    "  \n",
    " \n",
    "# Merge the dataframes based on the station name\n",
    "merged_df1 = pd.merge( results_PT, coordinates_df, left_on=\"Station\", right_on=\"Station\")\n",
    "# Define the path for the output file\n",
    "results_file_path1 = os.path.join(fig_folder, 'results_latlonPT.csv')\n",
    "merged_df1 = merged_df1.drop_duplicates(subset='Station')\n",
    "merged_df1.to_csv(results_file_path1, index=False)\n",
    "\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_file_path = os.path.join(fig_folder, 'results.csv')\n",
    "results_MK.to_csv(results_file_path, index=False)\n",
    "\n",
    "data_df =results_MK\n",
    "\n",
    "# Merge the dataframes based on the station name\n",
    "merged_df = pd.merge(data_df, coordinates_df, left_on=\"Station\", right_on=\"Station\")\n",
    "merged_df = merged_df.drop_duplicates(subset='Station')\n",
    "\n",
    "# Define the path for the output file\n",
    "results_file_path = os.path.join(fig_folder, 'results_latlonMK.csv')\n",
    "merged_df.to_csv(results_file_path, index=False)\n",
    "merged_df_reset = merged_df.reset_index(drop=True)\n",
    "mean_results_reset = mean_results.reset_index(drop=True)\n",
    "\n",
    "# Concatenating DataFrames side by side\n",
    "result_df_all = pd.concat([merged_df_reset, mean_results_reset], axis=1)\n",
    "# Creating a new column \"slope over CNP\"\n",
    "result_df_all['slope over CNP'] = abs((result_df_all['Slope'] *50))\n",
    "\n",
    "# Display the updated DataFrame\n",
    "#print(result_df_all)\n",
    "\n",
    "#print(result_df)\n",
    "# Define the path for the output file\n",
    "path = os.path.join(fig_folder, 'results_latlonMAF.csv')\n",
    "result_df_all.to_csv(path, index=False)\n",
    "\n",
    "merged_df11=result_df_all\n",
    "# Assuming 'Station' is the common column in both dataframes\n",
    "combined_MKnP = results_PT.merge(merged_df11, on='Station', how='outer')\n",
    "selected_columns = ['Station','Latitude', 'Longitude', 'Pettitt_K', 'P-Ptt','Trend SignificancePtt', 'Change value','Change Point','Trend Direction', 'Trend Significance', 'P-MK', 'Taub', 'Slope', 'Lower Bound','Upper Bound','Area','Mean','slope over CNP']\n",
    "MKnP_MAD =combined_MKnP[selected_columns]\n",
    "MKnP_MAD.to_csv(os.path.join(fig_folder1, \"MKnP_7dayminflowTime.csv\"), index=False)\n",
    "## plot\n",
    "sb.plot_trend_mapPtt(merged_df1, shapefile_path,fig_folder1, '7-day Minimum Flow Timing \\n (1971-2020) \\n Pettitt Significance', 'change_7dayflowTime.png')\n",
    "sb.plot_trend_mapMKT(merged_df11, shapefile_path,fig_folder1, '7-day Minimum Flow Timing \\n (1971-2020) \\n MK Significance', 'trend_7dayflowTime.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25f5158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1138284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02c88369",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Annual Coefficient of Variation\n",
    "    - Calculated by dividing the standard deviation of the annual streamflow values by the mean annual streamflow and expressed as a percentage.\n",
    "    - MKnP_ACV.csv: This file contains the results of Man kendall and Pettit test for ACV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65844475",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an empty DataFrame to store the results\n",
    "results_PT = pd.DataFrame(columns=['Station', 'Pettitt_K', 'P-Ptt','LowerBoundPtt','Trend SignificancePtt', 'Change value','Change Point'])\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "results_MK = pd.DataFrame(columns=['Station', 'Trend','Trend Direction', 'Trend Significance', 'Hypothesis', 'P-MK', 'Z-score', 'Taub', 'S', 'Variance of S', 'Slope', 'Intercept','Lower Bound','Upper Bound'])\n",
    "\n",
    "# Specify the path to the folder containing the discharge CSV files\n",
    "fig_folder = os.path.join(data_folder ,\"FigureAnnualCV7cate\")\n",
    "\n",
    "# Create the \"figure\" folder if it doesn't exist\n",
    "os.makedirs(fig_folder, exist_ok=True)\n",
    "\n",
    "fig_folderPTT = os.path.join(fig_folder ,\"PTT\")\n",
    "os.makedirs(fig_folderPTT, exist_ok=True)\n",
    "\n",
    "\n",
    "# Get a list of all CSV files in the data folder\n",
    "csv_files = [file for file in os.listdir(data_folder) if file.endswith(\".csv\")]\n",
    "\n",
    "# Loop through each CSV file in the folder\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(data_folder, csv_file)\n",
    "    \n",
    "    # Load the data from the CSV file\n",
    "    data = pd.read_csv(file_path)\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    data.set_index('Date', inplace=True)\n",
    "    \n",
    "    for period in periods:\n",
    "        start_year, end_year = period\n",
    "\n",
    "    # Select data for the specified time period\n",
    "        station_data = data.loc[(data.index.year >= start_year) & (data.index.year <= end_year)]\n",
    "        #print(station_data)\n",
    "        \n",
    "        station_number = csv_file.split('.')[0].upper() \n",
    "       # print(station_number)\n",
    "        station_data =  station_data[ station_data['STATION_NUMBER'] == station_number].copy()\n",
    "        \n",
    "          # Define the Water Year\n",
    "        station_data['Water_Year'] = station_data.index.where(station_data.index.month >= 10, station_data.index - pd.DateOffset(years=1)).year+1\n",
    "        \n",
    "        ####################################\n",
    "        \n",
    "            # Group by WaterYear\n",
    "        for water_year, group in station_data.groupby('Water_Year'):\n",
    "            # Extract the months for this WaterYear\n",
    "            months_in_water_year = set(group.index.month)\n",
    "\n",
    "            # Check if any month is missing\n",
    "            if months_in_water_year != expected_months:\n",
    "                # Replace only the 'value' column in this WaterYear group with NaN\n",
    "                station_data.loc[station_data['Water_Year'] == water_year, 'Value'] = np.nan\n",
    "        annual_cvn = (station_data['Value'].resample('A-SEP').apply(lambda x: x.dropna().std()) /station_data['Value'].resample('A-SEP').apply(lambda x: x.dropna().mean()))\n",
    "\n",
    "        annual_cv = annual_cvn.drop([annual_cvn.index[0], annual_cvn.index[-1]])\n",
    "\n",
    "      \n",
    "    trend, h, p, z, Tau, s, var_s, slope, intercept,lb,ub,Taub  = mk.yue_wang_modification_test(annual_cv,lag=1)\n",
    "\n",
    "    new_result = create_mk_result_entry(csv_file.replace('.csv', ''), trend, slope, h, p, z, Taub, s, var_s, lb, ub,intercept)\n",
    "    results_MK= pd.concat([results_MK, new_result], ignore_index=True)\n",
    "    \n",
    "    mask = ~np.isnan(annual_cv.values)\n",
    "\n",
    "    locP, K, p_value = mk.pettitt_dip(annual_cv.values[mask])\n",
    "    new_data = create_pettitt_result_entry(csv_file.replace('.csv', ''), K, p_value, locP,annual_cv)\n",
    "    results_PT = pd.concat([results_PT, new_data], ignore_index=True)\n",
    "    \n",
    "        ##################################################\n",
    "    # Calculate mean before and after the change point\n",
    "    change_point_year = annual_cv.index[int(locP)]\n",
    "    mean_before_change = annual_cv[annual_cv.index < change_point_year].mean()\n",
    "    mean_after_change = annual_cv[annual_cv.index >= change_point_year].mean()\n",
    "    \n",
    "#     print(mean_before_change, mean_after_change)\n",
    "  \n",
    "    \n",
    "    plt.figure(figsize=(5, 4), facecolor='white')\n",
    "    plt.scatter(annual_cv.index, annual_cv, marker=\"o\", color=\"black\", s=8)\n",
    "    plt.plot(annual_cv.index, slope * np.arange(len(annual_cv)) + intercept, color='red',label='Trend Line')\n",
    "    plt.axvline(annual_cv.index[int(locP)], color='b', linestyle='--', label='Change Point')\n",
    "    \n",
    "    # Reduce the size of tick marks on x and y axes\n",
    "    plt.xticks(fontsize=6)\n",
    "    plt.yticks(fontsize=6)\n",
    "  \n",
    "    # Plot horizontal line for the mean before the change point\n",
    "    plt.hlines(y=mean_before_change, xmin=annual_cv.index.min(), xmax=change_point_year,color='green', linestyle='-', label=f'Mean Before ({mean_before_change:.3f})')\n",
    "\n",
    "    # Plot horizontal line for the mean after the change point\n",
    "    plt.hlines(y=mean_after_change, xmin=change_point_year, xmax=annual_cv.index.max(),color='green', linestyle='-', label=f'Mean After ({mean_after_change:.3f})')\n",
    "\n",
    "\n",
    "    plt.xlabel(\"Water Year\",fontsize=6)\n",
    "    plt.ylabel('Discharge (m^3/sec)',fontsize=6)\n",
    "#     plt.title(f\"ACV - Station {station_number} and Change Point ({start_year+1}-{end_year})\",fontsize=6)\n",
    "    plt.title(f\"ACV - Station {station_number})\",fontsize=6)\n",
    "        \n",
    "    textstr = f\"p-MK: {p:.3f}\\nTau: {Tau:.3f}\\nSens-Slope: {slope:.3f}\"\n",
    "    props = dict(boxstyle='round', facecolor='white', alpha=0.5, edgecolor='none')\n",
    "    plt.text(1.01, 0.95, textstr, transform=plt.gca().transAxes, fontsize=6, verticalalignment='top', bbox=props)\n",
    "        \n",
    "    textstr = f\"p-Pettitt: {p_value:.3f}\\nChange point (CP): {annual_cv.index[int(locP)]}\\nMean before CP:{mean_before_change:.2f}\\nMean after CP:{mean_after_change:.2f}\"\n",
    "    props = dict(boxstyle='round', facecolor='white', alpha=0.5, edgecolor='none')\n",
    "    plt.text(1.01, 0.80, textstr, transform=plt.gca().transAxes, fontsize=6, verticalalignment='top', bbox=props)\n",
    "        \n",
    "    plt.legend(['Discharge Data', 'Trend Line', 'Change Point'], bbox_to_anchor=(1.01, 0), loc='lower left',fontsize=6)\n",
    "\n",
    "    output_plot_filename = os.path.join(fig_folderPTT, f\"Station_{station_number}_plot_{start_year+1}_{end_year}.png\")\n",
    "    plt.savefig(output_plot_filename, bbox_inches='tight', dpi=300)\n",
    "\n",
    "\n",
    "    ##################################################\n",
    " \n",
    "# Merge the dataframes based on the station name\n",
    "merged_df1 = pd.merge( results_PT, coordinates_df, left_on=\"Station\", right_on=\"Station\")\n",
    "# Define the path for the output file\n",
    "results_file_path1 = os.path.join(fig_folder, 'results_latlonPT.csv')\n",
    "merged_df1 = merged_df1.drop_duplicates(subset='Station')\n",
    "merged_df1.to_csv(results_file_path1, index=False)\n",
    "\n",
    "data_df =results_MK\n",
    "\n",
    "# Merge the dataframes based on the station name\n",
    "merged_df = pd.merge(data_df, coordinates_df, left_on=\"Station\", right_on=\"Station\")\n",
    "merged_df = merged_df.drop_duplicates(subset='Station')\n",
    "\n",
    "# Define the path for the output file\n",
    "results_file_path = os.path.join(fig_folder, 'results_latlonMK.csv')\n",
    "merged_df.to_csv(results_file_path, index=False)\n",
    "merged_df_reset = merged_df.reset_index(drop=True)\n",
    "mean_results_reset = mean_results.reset_index(drop=True)\n",
    "\n",
    "# Concatenating DataFrames side by side\n",
    "result_df_all = pd.concat([merged_df_reset, mean_results_reset], axis=1)\n",
    "# Creating a new column \"Slope as % of MAF\"\n",
    "result_df_all['Slope as % of MAF'] = abs((result_df_all['Slope'] / result_df_all['Mean']) * 100)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "#print(result_df_all)\n",
    "\n",
    "#print(result_df)\n",
    "# Define the path for the output file\n",
    "path = os.path.join(fig_folder, 'results_latlonMAF.csv')\n",
    "result_df_all.to_csv(path, index=False)\n",
    "\n",
    "merged_df11=result_df_all\n",
    "# Assuming 'Station' is the common column in both dataframes\n",
    "combined_MKnP = results_PT.merge(merged_df11, on='Station', how='outer')\n",
    "selected_columns = ['Station','Latitude', 'Longitude', 'Pettitt_K', 'P-Ptt','Trend SignificancePtt', 'Change value','Change Point','Trend Direction', 'Trend Significance', 'P-MK', 'Taub', 'Slope', 'Lower Bound','Upper Bound','Area','Mean','Slope as % of MAF']\n",
    "MKnP_MAD =combined_MKnP[selected_columns]\n",
    "MKnP_MAD.to_csv(os.path.join(fig_folder1, \"MKnP_ACV.csv\"), index=False)\n",
    "   \n",
    "## plot\n",
    "sb.plot_trend_mapPtt(merged_df1,shapefile_path,fig_folder1, 'Annual Coefficient of Variation \\n (1971-2020) \\n Pettitt Significance', 'change_ACV.png')\n",
    "sb.plot_trend_mapMK(merged_df11,shapefile_path,fig_folder1, 'Annual Coefficient of Variation \\n (1971-2020) \\n MK Significance', 'trend_ACV.png')   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3a430d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e37b159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acd252e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfc3a86e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Richards Baker Flashiness Index\n",
    "    - Calculated by the summing of the differences between flows on two consecutive data points and then dividing by the total average flow for a year.\n",
    "    - MKnP_RBI.csv: This file contains the results of Man kendall and Pettit test for RBI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927e65f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an empty DataFrame to store the results\n",
    "results_PT = pd.DataFrame(columns=['Station', 'Pettitt_K', 'P-Ptt','Trend SignificancePtt', 'Change value','Change Point'])\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "results_MK = pd.DataFrame(columns=['Station', 'Trend','Trend Direction', 'Trend Significance', 'Hypothesis', 'P-MK', 'Z-score', 'Taub', 'S', 'Variance of S', 'Slope', 'Intercept','Lower Bound','Upper Bound'])\n",
    "\n",
    "# Specify the path to the folder containing the discharge CSV files\n",
    "fig_folder = os.path.join(data_folder ,\"FigureRBI7cate\")\n",
    "\n",
    "# Create the \"figure\" folder if it doesn't exist\n",
    "os.makedirs(fig_folder, exist_ok=True)\n",
    "\n",
    "fig_folderPTT = os.path.join(fig_folder ,\"PTT\")\n",
    "os.makedirs(fig_folderPTT, exist_ok=True)\n",
    "\n",
    "# Get a list of all CSV files in the data folder\n",
    "csv_files = [file for file in os.listdir(data_folder) if file.endswith(\".csv\")]\n",
    "\n",
    "# Loop through each CSV file in the folder\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(data_folder, csv_file)\n",
    "    \n",
    "    # Load the data from the CSV file\n",
    "    data = pd.read_csv(file_path)\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    data.set_index('Date', inplace=True)\n",
    "    \n",
    "    for period in periods:\n",
    "        start_year, end_year = period\n",
    "\n",
    "    # Select data for the specified time period\n",
    "        station_data = data.loc[(data.index.year >= start_year) & (data.index.year <= end_year)]\n",
    "        #print(station_data)\n",
    "        \n",
    "        station_number = csv_file.split('.')[0].upper() \n",
    "       # print(station_number)\n",
    "        station_data =  station_data[ station_data['STATION_NUMBER'] == station_number].copy()\n",
    "    \n",
    " \n",
    "          # Define the Water Year\n",
    "        station_data['WaterYear'] = station_data.index.where(station_data.index.month >= 10, station_data.index - pd.DateOffset(years=1)).year+1\n",
    "        \n",
    "        ####################################\n",
    "        \n",
    "            # Group by WaterYear\n",
    "        for water_year, group in station_data.groupby('WaterYear'):\n",
    "            # Extract the months for this WaterYear\n",
    "#             months_in_water_year = set(group['Date'].dt.month)\n",
    "            months_in_water_year = set(group.index.month)\n",
    "#             print(months_in_water_year)\n",
    "\n",
    "            # Check if any month is missing\n",
    "            if months_in_water_year != expected_months:\n",
    "                # Replace only the 'value' column in this WaterYear group with NaN\n",
    "                station_data.loc[station_data['WaterYear'] == water_year, 'Value'] = np.nan\n",
    "#                 print(station_data['Value'])\n",
    "\n",
    "    \n",
    "\n",
    "# Calculate the Richards-Baker Index over the annual period\n",
    "    station_data['RBI'] = station_data.groupby('WaterYear')['Value'].apply(lambda group: group.diff().abs().sum() / group.sum())\n",
    "\n",
    "#     rb_indexn =station_data.groupby('WaterYear')['Value'].apply(lambda group: group.diff().abs().sum() / group.sum())\n",
    "    rb_indexn = station_data.groupby('WaterYear')['Value'].apply(lambda group: group.dropna().diff().abs().sum() / group.dropna().sum())\n",
    "\n",
    "    rb_index = rb_indexn.drop([rb_indexn.index[0], rb_indexn.index[-1]])\n",
    "\n",
    "    \n",
    "    \n",
    "#     print(rb_index)\n",
    "    \n",
    "\n",
    "    # Perform Mann-Kendall test on station_data_RBI_resampled\n",
    "\n",
    "    trend, h, p, z, Tau, s, var_s, slope, intercept,lb,ub,Taub  = mk.yue_wang_modification_test(rb_index,lag=1)\n",
    "    new_result = create_mk_result_entry(csv_file.replace('.csv', ''), trend, slope, h, p, z, Taub, s, var_s, lb, ub,intercept)\n",
    "    results_MK = pd.concat([results_MK, new_result], ignore_index=True)\n",
    "    \n",
    "    mask = ~np.isnan(rb_index.values)\n",
    "\n",
    "    locP, K, p_value = mk.pettitt_dip(rb_index.values[mask])\n",
    "    new_data = create_pettitt_result_entry(csv_file.replace('.csv', ''), K, p_value, locP, rb_index)\n",
    "    results_PT = pd.concat([results_PT, new_data], ignore_index=True)\n",
    "        ##################################################\n",
    "    # Calculate mean before and after the change point\n",
    "    change_point_year = rb_index.index[int(locP)]\n",
    "    mean_before_change = rb_index[rb_index.index < change_point_year].mean()\n",
    "    mean_after_change = rb_index[rb_index.index >= change_point_year].mean()\n",
    "    \n",
    "#     print(mean_before_change, mean_after_change)\n",
    "  \n",
    "    \n",
    "    plt.figure(figsize=(5, 4), facecolor='white')\n",
    "    plt.scatter(rb_index.index, rb_index, marker=\"o\", color=\"black\", s=8)\n",
    "    plt.plot(rb_index.index, slope * np.arange(len(rb_index)) + intercept, color='red',label='Trend Line')\n",
    "    plt.axvline(rb_index.index[int(locP)], color='b', linestyle='--', label='Change Point')\n",
    "    \n",
    "    # Reduce the size of tick marks on x and y axes\n",
    "    plt.xticks(fontsize=6)\n",
    "    plt.yticks(fontsize=6)\n",
    "  \n",
    "    # Plot horizontal line for the mean before the change point\n",
    "    plt.hlines(y=mean_before_change, xmin=rb_index.index.min(), xmax=change_point_year,color='green', linestyle='-', label=f'Mean Before ({mean_before_change:.3f})')\n",
    "\n",
    "    # Plot horizontal line for the mean after the change point\n",
    "    plt.hlines(y=mean_after_change, xmin=change_point_year, xmax=rb_index.index.max(),color='green', linestyle='-', label=f'Mean After ({mean_after_change:.3f})')\n",
    "\n",
    "\n",
    "    plt.xlabel(\"Water Year\",fontsize=6)\n",
    "    plt.ylabel('Discharge (m^3/sec)',fontsize=6)\n",
    "    plt.title(f\"RBI - Station {station_number}\",fontsize=6)    \n",
    "        \n",
    "    textstr = f\"p-MK: {p:.3f}\\nTau: {Tau:.3f}\\nSens-Slope: {slope:.3f}\"\n",
    "    props = dict(boxstyle='round', facecolor='white', alpha=0.5, edgecolor='none')\n",
    "    plt.text(1.01, 0.95, textstr, transform=plt.gca().transAxes, fontsize=6, verticalalignment='top', bbox=props)\n",
    "        \n",
    "    textstr = f\"p-Pettitt: {p_value:.3f}\\nChange point (CP): {rb_index.index[int(locP)]}\\nMean before CP:{mean_before_change:.2f}\\nMean after CP:{mean_after_change:.2f}\"\n",
    "    props = dict(boxstyle='round', facecolor='white', alpha=0.5, edgecolor='none')\n",
    "    plt.text(1.01, 0.80, textstr, transform=plt.gca().transAxes, fontsize=6, verticalalignment='top', bbox=props)\n",
    "        \n",
    "    plt.legend(['Discharge Data', 'Trend Line', 'Change Point'], bbox_to_anchor=(1.01, 0), loc='lower left',fontsize=6)\n",
    "\n",
    "    output_plot_filename = os.path.join(fig_folderPTT, f\"Station_{station_number}_plot_{start_year+1}_{end_year}.png\")\n",
    "    plt.savefig(output_plot_filename, bbox_inches='tight', dpi=300)\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "    ##################################################\n",
    "         \n",
    "\n",
    "\n",
    "\n",
    "# Merge the dataframes based on the station name\n",
    "merged_df1 = pd.merge( results_PT, coordinates_df, left_on=\"Station\", right_on=\"Station\")\n",
    "# Define the path for the output file\n",
    "results_file_path1 = os.path.join(fig_folder, 'results_latlonPT.csv')\n",
    "merged_df1 = merged_df1.drop_duplicates(subset='Station')\n",
    "merged_df1.to_csv(results_file_path1, index=False)\n",
    "\n",
    "\n",
    "data_df =results_MK\n",
    "\n",
    "# Merge the dataframes based on the station name\n",
    "merged_df = pd.merge(data_df, coordinates_df, left_on=\"Station\", right_on=\"Station\")\n",
    "merged_df = merged_df.drop_duplicates(subset='Station')\n",
    "\n",
    "# Define the path for the output file\n",
    "results_file_path = os.path.join(fig_folder, 'results_latlonMK.csv')\n",
    "merged_df.to_csv(results_file_path, index=False)\n",
    "merged_df_reset = merged_df.reset_index(drop=True)\n",
    "mean_results_reset = mean_results.reset_index(drop=True)\n",
    "\n",
    "# Concatenating DataFrames side by side\n",
    "result_df_all = pd.concat([merged_df_reset, mean_results_reset], axis=1)\n",
    "# Creating a new column \"Slope as % of MAF\"\n",
    "result_df_all['Slope as % of MAF'] = abs((result_df_all['Slope'] / result_df_all['Mean']) * 100)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "#print(result_df_all)\n",
    "\n",
    "#print(result_df)\n",
    "# Define the path for the output file\n",
    "path = os.path.join(fig_folder, 'results_latlonMAF.csv')\n",
    "result_df_all.to_csv(path, index=False)\n",
    "\n",
    "merged_df11=result_df_all\n",
    "# Assuming 'Station' is the common column in both dataframes\n",
    "combined_MKnP = results_PT.merge(merged_df11, on='Station', how='outer')\n",
    "selected_columns = ['Station','Latitude', 'Longitude', 'Pettitt_K', 'P-Ptt','Trend SignificancePtt', 'Change value','Change Point','Trend Direction', 'Trend Significance', 'P-MK', 'Taub', 'Slope', 'Lower Bound','Upper Bound','Area','Mean','Slope as % of MAF']\n",
    "MKnP_MAD =combined_MKnP[selected_columns]\n",
    "MKnP_MAD.to_csv(os.path.join(fig_folder1, \"MKnP_RBI.csv\"), index=False)\n",
    "   \n",
    "## plot\n",
    "sb.plot_trend_mapPtt(merged_df1,shapefile_path,fig_folder1, 'Richards Baker Index \\n (1971-2020) \\n Pettitt Significance', 'change_RBI.png')\n",
    "sb.plot_trend_mapMK(merged_df11,shapefile_path,fig_folder1, 'Richards Baker Index \\n (1971-2020) \\n MK Significance', 'trend_RBI.png')   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d0803b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
